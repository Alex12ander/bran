export MODEL_NAME='bcVI_t5/genia_sentence_filtered'
export train=${CDR_IE_ROOT}/src/train_multiclass_classifier.py

data_root=${CDR_IE_ROOT}/data/BC_VI_Task5/processed_genia_sentence_filtered/protos
export vocab_dir=${data_root}

export positive_train=${data_root}/positive_0_CDR_train.txt.proto
export negative_train=${data_root}/negative_0_CDR_train.txt.proto
export positive_test=${data_root}/positive_0_CDR_train_dev.txt.proto
export negative_test=${data_root}/negative_0_CDR_train_dev.txt.proto
# export positive_test_test=${data_root}/positive_\*test.txt.proto
# export negative_test_test=${data_root}/negative_\*test_filtered.txt.proto

export ner_train=${data_root}/ner_CDR_train.txt.proto
export ner_test=${data_root}/ner_CDR_dev.txt.proto
export ner_prob=0.5
export ner_weight=10.0

# export doc_filter=${CDR_IE_ROOT}/data/cdr/CDR_pubmed_ids/CDR_Train_Dev_pubmed_ids.txt
# export train_dev_percent=.85

export logdir=$LOG_DIR
export optimizer=adam
export model_type=classifier
export embeddings=${CDR_IE_ROOT}/embeddings/PubMed-and-PMC-w2v.txt

export bidirectional=True
export in_memory=True
export text_encoder=transformer_cnn_all_pairs
export num_classes=6
export kb_vocab_size=6
export block_repeats=2

export null_label=0

export lr=.0005
export margin=1.0
export l2_weight=0
export clip_norm=10
export dropout_loss_weight=0

export text_weight=1.0
export text_prob=1.0

export word_unk_dropout=0.85
export word_dropout=0.5
export lstm_dropout=1.0
export final_dropout=.85

export epsilon=1e-8
export beta1=.1
export beta2=.9
export noise_std=1.0

export text_batch=32
export kb_batch=32
export ner_batch=8
export eval_batch=8

export token_dim=200
export lstm_dim=200
export embed_dim=200
export position_dim=0

export pos_noise=.33
export neg_noise=.20
# export percentile=True

export kb_pretrain=0
export kb_epochs=100000
export text_epochs=100000
export eval_every=33000
export max_seq=2000
export neg_samples=200
export random_seed=1111

# saved_models/bcVI_t5/genia_sentence_filtered/2017-10-24-13/.0005_8_32_10_0.5_1.0_1.0_0_0_1.0_200_0_0_0_1.0_.85_.5_.85_1.0_1e-8_.1_.9_25000_1.0_0_.5_2_-max_decrease_epochs_7_--embeddings_embeddings-PubMed-and-PMC-w2v.txt/train.log   dev: 49.8434    test: 0 ner: 0  count: 3
